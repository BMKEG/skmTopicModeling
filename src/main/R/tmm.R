require("igraph")
require('flexmix')

# Reads topic model vectors from files like the ones 
# generated from a mallet model by the program
# edu.isi.bmkeg.skm.topicmodeling.bin.DumpTopicVectors
# Arguments: 
#    dtvFile: is a text file with one line per document
#        containing the topics proportions vector of that
#        document. Each line contains k real numbers separated by
#        tabs corresponding to the proportions of each one of the k
#        topics (from 1 to k).
#    idsFile: is a text file with one line per document 
#        containing the document's id. The order in which 
#        ids are listed in the file has to correspond to
#        the order in which the document' topics proportions 
#        vectors are listed in the file passed in the dtvFile argument. 

read.topicVectors <- function(idsFile, dtvFile) {
    # Reads dtvFile
	c <- scan(dtvFile)
	
	# Reads idsFile
	ids <- scan(idsFile)
	
	# computes number of topics
	n <- length(c) %/% length(ids)
	
	# Returns topics Vector matrix
	matrix(c, ncol = n, byrow=TRUE, 
		dimnames = list(documents=ids,topics=1:n))
}

read.topicWords <- function(file) {
	t <- read.table(file,sep="\t",stringsAsFactors=FALSE,quote="")	
	
	tw <- t$V3
	names(tw) <- 1:length(tw)
	
	return(tw)
}

write.diagTopics <- function(topicWords, diagTopics,file) {
	t <- list(cluster=1:length(diagTopics),diagTopic=diagTopics,words=topicWords[diagTopics])
	write.table(t,file,quote=FALSE,sep='\t',row.name=FALSE,col.name=TRUE)
}

# c2c is a matrix as returned by the function closestToCenter
write.closestNodes <- function(c2c, file) {
  t <- list(cluster=1:nrow(c2c),docs=c2c)
  write.table(t,file,quote=FALSE,sep='\t',row.name=FALSE,col.name=TRUE)
}

write.closestNodesWithTopics <- function(l,cl,tv,ndocs,ntopics,file) {
  
  # vector with documents closest ti the cluster centers, sorted by distance and cluster
  c2c <- as.vector(t(closestToCenter(n=ndocs,l=l,cl=cl)))

  # Top Topics for the documents in c2c
  tdt <- topDocTopics(tv=tv[c2c,],n=ntopics)
  
  f <- file(description=file,open="w")
  cat("vpdmfid\tcluster\t(topic, prop) ...\n",file=f)
  
  for (i in 1:length(c2c)) {
    docid <- c2c[i]
    cat(docid,'\t',cl$cluster[docid],file=f)

    topics <- tdt[[i]][[1]]
    for (j in 1:length(topics)) {
      cat('\t',names(topics)[j],'\t',topics[j],file=f)
    }
    cat('\n',file=f)
  }
  close(f)
}

write.clustersTopTopics <- function(cl,tv,ntopics,file) {
  
  # Mean topic vector per cluster
  clTv <- t(sapply(1:nrow(cl$centers), function(y,tv,cl) {colMeans(tv[which(cl$cluster == y),])},tv,cl))

  # Top Topics per cluster
  clTopTopics <- topDocTopics(clTv, ntopics)

  f <- file(description=file,open="w")

  cat("cluster\t(topic, prop) ...\n",file=f)
  
  for (i in 1:length(clTopTopics)) {
    cat(i,'\t',file=f)
    
    topics <- clTopTopics[[i]][[1]]
    for (j in 1:length(topics)) {
      cat('\t',names(topics)[j],'\t',topics[j],file=f)
    }
    cat('\n',file=f)
  }
  close(f)
}

# MAIN FUNCTION
# Arguments:
#   similarityGraphFile: a pairwise document similarity graphml file
#       as generated by edu.isi.bmkeg.skm.topicmodeling.bin.
#       ComputePairwiseDocumentSimilarities.
#   idsFile (see function read.topicVectors for a description of this argument)
#   dtvFile (see function read.topicVectors for a description of this argument)
#   edgecut: number between 0 and 1 to be passed as the edge.cut parameter
#       of the layout.drl function.
#   cntClusters: number of clusters to compute in the DrL layout
#   ... : other parameters to be passed to the plot() function
#
# Value:
#    a vector with the diagnostic topics per cluster
#
plotTMM.file <- function (similarityGraph, idsFile, dtvFile, edgecut = 0.8,
				cntClusters,sub=sprintf("edgecut: %.1f cntClusters: %g", edgecut, 
		cntClusters),...) {

	require("igraph")
	
	cat(date(), ': loading similarity graph from file ...\n')
	g <- read.graph(similarityGraph, format='graphml')

	cat(date(), ': loading topic vectors from files ...\n')
	tv <- read.topicVectors(idsFile, dtvFile)
	
	cat(date(), ': computing DrL layout from similarity graph ...\n')
	l <- computeLayout(g, edgecut)
	cat(date(), 'Done computing DrL layout\n')

	plotTMM(l, tv, cntClusters, ... )

}

computeLayout <- function(g, edgecut) {
  require('igraph')
  
	# computing DrL layout from similarity graph
	l <- layout.drl(g, options=list(edge.cut= edgecut))

	# Adding dimnames to layout
 	dimnames(l) <- list(documents=V(g)$name, coord=c('x','y'))

	return(l)
}

# Selects the top topic (i.e., the topic with highest proportion) of each topic vector
# Arguments:
#     tv: an array of topic vectors as a matrix.
#
# Value: a vector with the top topic of each topic vector in tv.
#
topTopics <- function(tv) {
	apply(tv,MARGIN=1, FUN=which.max)
}

# Selects the diagnostic topic of each topic vector
# Arguments:
#     ctv: an array of cluster topic vectors (one topic vector per cluster) as a matrix.
#     tv: the documents topic vectors
#
# Value: a vector with the diagnostoc topics of each topic vector in ctv.
#
diagTopics <- function(ctv, tv) {
	
	# Computes Mean Topic Vector across all documents topic vectors
	mtv <- meanTopicProp(tv)
	
	# Computes proportions ctv relative to mtv
	rctv <- t(ctv)/mtv
	 	
	apply(rctv,MARGIN=2, FUN=which.max)
}

diagTopicsWeights <- function(ctv, clt) {
	
	ctv[cbind(1:nrow(ctv),clt)]
}

diagTopicsRelWeights <- function(ctv,clt,tv) {
	
	# Computes Mean Topic Vector across all documents topic vectors
	mtv <- meanTopicProp(tv)

	# Computes proportions ctv relative to mtv
	rctv <- t(ctv)/mtv
	 	
	rctv[cbind(clt,1:nrow(ctv))]
}

# Computes distance between a node position in a layout to its cluster center
# Arguments:
#     l: nodes layout (as returned by the layout.drl() function)
#     cl: clusters over l computed using kmeans
# Value:
#     A vector with the euclidean distances
distToCenter <- function(l,cl) {
	euclidDist(l[,1],l[,2],cl$centers[cl$cluster,1],cl$centers[cl$cluster,2])
}

# Computes distance between the position of all nodes of a cluster to its center
# Arguments:
#     k: cluster number
#     l: nodes layout (as returned by the layout.drl() function)
#     cl: clusters over l computed using kmeans
# Value:
#     A vector with the euclidean distances
singleClusterDistToCenter <- function(k,l,cl) {
	i <- which(cl$cluster == k)
	euclidDist(l[i,1],l[i,2],cl$centers[k,1],cl$centers[k,2])
}

# Finds the n closest nodes to the center of each cluster
# Arguments:
#     n: Number of nodes per cluster in result
#     l: nodes layout (as returned by the layout.drl() function)
#     cl: clusters over l computed using kmeans
# Value:
#     A matrix with n columns and one row per cluster containing the names of
#     the closest nodes to the center of each cluster
closestToCenter <- function(n, l, cl) {
  d2c <- distToCenter(l,cl)
  
  docs <- sapply(1:nrow(cl$centers), 
                 function(k) {
                   names(sort(d2c[which(cl$cluster == k)])[1:n])
                 })
  return(t(docs))
}

# Arguments:
#  tv: matrix with document/topic proportions (one document per row, one topic per column)
#  n: number of proportions per document to return
#
# Value: a named document list of named list of proportions. Each element of the outer list is named with a document name,
#   and each element in the inner list is named with the topic number.
topDocTopics <- function(tv,n) {
  apply(tv,MARGIN=1,
        function(x){
          top <- sort(x,decreasing=TRUE)[1:n]
          list(top)
        })
}

plotTMM <- function (l, tv, cntClusters, nstart = 1, iter.max  =10, ... ) {
	
	cat(date(), ': computing clusters in the documents DrL layout ... \n')
	cl <- kmeans(l, centers = cntClusters, nstart = nstart, iter.max = iter.max)
	
	cat(date(), ': computing mean topic vectors per cluster ...\n')
	clTv <- t(sapply(1:nrow(cl$centers), function(y,tv,cl) {colMeans(tv[which(cl$cluster == y),])},tv,cl))
	
	cat(date(), ': finding diagnostic topic per cluster ...\n')
	clt <- diagTopics(clTv,tv)
	
	cat(date(), ': ploting layout ...\n')
	plot(l, pch=20, cex=.1, col=cl$cluster,...)

	cat('printing labels ... \n')	
	text(x=cl$centers[,1],y=cl$centers[,2], labels = clt, pos=1, cex=1)
	
	cat(date(), ': End.')	
	
	return(list(cl = cl, clt = clt))
}

# Computes mean proportions for each topic in the corpus
meanTopicProp <- function(tv) {
	colMeans(tv)
}

# Counts how many edges weighting more than a threshold a pairwise 
# document similarities graph would have (cosine similarities).
# Arguments:
#    tv: document topic vectors matrix
#    threshold: similarity score cutoff threshold. Only
#       edges over this value will be included in the result.
#
# Value: a number
#
countEdges <- function(tv, threshold = 0.1) {
		
	cnt <- 0
    for (i in 1:(nrow(tv) - 1)) {
    	for (j in (i + 1):nrow(tv)) {
    		
    		c <- myCosine(tv[i,], tv[j,])
    	  
			if (c >= threshold) {
			
				cnt <- cnt + 1	
	
			}

    	}
    }
    
    total <- (nrow(tv) * (nrow(tv) -1)) / 2
    cat(sprintf("%d out of %d edges heavier than %f (%.2f percentile) \n",cnt,total,threshold, 1 - cnt/total))
	return(cnt)

 }	

# Computes pairwise document similarities (cosine)
# Arguments:
#    tv: document topic vectors matrix
#    threshold: similarity score cutoff threshold. Only
#       edges over this value will be included in the result.
#
# Value: a weighted undirected igraph whose vertices correspond to
#       documents and whose edges weight are the Cosine similarity scores.
#
docSimilarities <- function(tv, threshold = 0.1, quadSize = 5000L) {
	
  require("igraph")

	cat(date(), ': Computing cosine distances \n')

	e <- findEdges(tv, quadSize = as.integer(quadSize), threshold = threshold)
	
	cat(date(), ': Constructing similarity graph \n')
	
	g <- graph.empty(directed=FALSE) + vertices(rownames(tv))

    g <- g + edges(as.vector(rbind(e$source,e$target)),weight=e$weight)
	
    cnt <- length(e$source)

    total <- (nrow(tv) * (nrow(tv) -1)) / 2
    cat(sprintf("%d out of %d edges included (%.2f percentile) \n",cnt,total, 1 - cnt/total))
    cat(sprintf("%d components \n",no.clusters(g)))

	cat(date(), '\n')

    return(g)
	
}

# Computes pairwise document similarities (Kullback-Leibler distance)
# Arguments:
#    tv: document topic vectors matrix
#    threshold: similarity score cutoff threshold. Only
#       edges over this value will be included in the result.
#
# Value: a weighted undirected igraph whose vertices correspond to
#       documents and whose edges weight are the similarity scores.
#
docSimilaritiesKl <- function(tv, eps = 10^-4, threshold = 0.1, quadSize = 5000L) {
  
  require("igraph")
  
  cat(date(), ': Computing cosine distances \n')
  
  e <- findEdgesKl(tv, eps, quadSize = as.integer(quadSize), threshold = threshold)
  
  cat(date(), ': Constructing similarity graph \n')
  
  g <- graph.empty(directed=FALSE) + vertices(rownames(tv))
  
  g <- g + edges(as.vector(rbind(e$source,e$target)),weight=e$weight)
  
  cnt <- length(e$source)
  
  total <- (nrow(tv) * (nrow(tv) -1)) / 2
  cat(sprintf("%d out of %d edges included (%.2f percentile) \n",cnt,total, 1 - cnt/total))
  cat(sprintf("%d components \n",no.clusters(g)))
  
  cat(date(), '\n')
  
  return(g)
  
}

# Computes a symetric KL similarity between two probability distributions.
KLsim3 <- function (x,y,logx=log(x),logy=log(y),  eps = 10^-4) {
  
  ok <- (x > eps) & (y > eps)
  if (any(ok)) {
    kl1 <-  sum(x * (logx - logy))
    kl2 <-  sum(y * (logy - logx))
    return(2/(kl1 + kl2))    
  } else {
    return(0)
  }
}

myKLsimM4 <- function(tv, tvlogs = log(tv),  eps = 10^-4, sourceFrom = 1L, targetFrom = 1L, max = 5000) {
  
  sourceTo <- min(sourceFrom + max - 1, nrow(tv))
  targetTo <- min(targetFrom + max - 1, nrow(tv))
  
  x <- tv[sourceFrom:sourceTo,]
  xlogs <- tvlogs[sourceFrom:sourceTo,]
  
  if (sourceFrom == sourceTo) {
    x <- rbind(x)
    xlogs <- rbind(xlogs)
  }
  
  y <- tv[targetFrom:targetTo,]
  ylogs <- tvlogs[targetFrom:targetTo,]
  
  if (targetFrom == targetTo) {
    y <- rbind(y)
    ylogs <- rbind(ylogs)
  }
  
  xlogy <- tcrossprod(x, ylogs)
  xlogx <- rowSums(x * xlogs)
  negKlxy <- sweep(xlogy,MARGIN=1,STATS=xlogx)
  
  ylogx <- tcrossprod(y, xlogs)
  ylogy <- rowSums(y * ylogs)
  negKlyx <- sweep(ylogx,MARGIN=1,STATS=ylogy)
  
  return(((negKlxy + t(negKlyx)) ^-1)* (-2))
}

myKLsimM3 <- function(tv, tvlogs=log(tv),  eps = 10^-4, sourceFrom = 1L, targetFrom = 1L, max = 5000) {
  
  sourceTo <- min(sourceFrom + max - 1, nrow(tv))
  targetTo <- min(targetFrom + max - 1, nrow(tv))
  
  # Computes KL distances between given tv rows
  
  sapply(X=targetFrom:targetTo,
         FUN=function(i){
           sapply(X=sourceFrom:sourceTo,
                  FUN=function(j){KLsim3(x=tv[i,],y=tv[j,],logx=tvlogs[i,],logy=tvlogs[j,],eps=eps)})
         })
  
}

findQuadEdgesKl <- function(tv, tvlogs = log(tv),  eps = 10^-4, sourceFrom = 1L, targetFrom = 1L, max = 5000L, threshold = 0.1) {
  
  cat(sprintf("Computing Quadrant (sourceFrom, targetFrom, max): %d %d %d \n", sourceFrom, targetFrom,max))
  
  # Computes cosine distances of given tv rows
  # 	tvcos <- myCosineM(tv, sourceFrom = sourceFrom, targetFrom = targetFrom, max = max)
  tvcos <- myKLsimM4(tv, tvlogs, eps, sourceFrom = sourceFrom, targetFrom = targetFrom, max = max)
  
  # Extracts indices and values for distances grater or equal than threshold
  thrIdx <- which(tvcos >= threshold)
  re <- row(tvcos)[thrIdx] + sourceFrom - 1L
  ce <- col(tvcos)[thrIdx] + targetFrom - 1L
  we <- tvcos[thrIdx]
  
  # Because edges are undirected we want only edges corresponding
  # to the upper triangle of the distance matrix
  lowerIndx <- which(re > ce)
  return(list(source=re[lowerIndx], target=ce[lowerIndx], weights=we[lowerIndx]))
}

findEdgesKl <- function(tv, eps = 10^-4, quadSize = 5000L, threshold = 0.1) {
  
  w <- tv < eps
  if (any(w)) 
    tv[w] <- eps
  tv <- sweep(tv, 1, rowSums(tv), "/")
  
  rm(w)
  tvlogs <- log(tv)
  
  quadSize <- as.integer(quadSize)
  re <- lapply(seq(from=1L,to=nrow(tv),by=quadSize), 
               FUN=function(i) {
                 ce <- lapply(seq(from=1L, to=i, by=quadSize),
                              FUN=function(j) {
                                findQuadEdgesKl(tv,tvlogs,sourceFrom=i, targetFrom=j, max=quadSize, threshold = threshold)
                              })
                 cat('Combining edges column ....')
                 ces<-combineEdges(ce)
                 cat('Done. \n')
                 return(ces)
               })
  cat('Combining edges row ....')  
  res <- combineEdges(re)
  cat('Done. \n')
  return(res)
}

findEdges <- function(tv, quadSize = 5000L, threshold = 0.1) {
	quadSize <- as.integer(quadSize)
	re <- lapply(seq(from=1L,to=nrow(tv),by=quadSize), 
			FUN=function(i) {
				ce <- lapply(seq(from=1L, to=i, by=quadSize),
						FUN=function(j) {
							findQuadEdges(tv,sourceFrom=i, targetFrom=j, max=quadSize, threshold = threshold)
						})
				cat('Combining edges column ....')
				ces<-combineEdges(ce)
				cat('Done. \n')
				return(ces)
			})
	cat('Combining edges row ....')	
	res <- combineEdges(re)
	cat('Done. \n')
	return(res)
}


findQuadEdges <- function(tv, sourceFrom = 1L, targetFrom = 1L, max = 5000L, threshold = 0.1) {
	
	cat(sprintf("Computing Quadrant (sourceFrom, targetFrom, max): %d %d %d \n", sourceFrom, targetFrom,max))
	
	# Computes cosine distances of given tv rows
	tvcos <- myCosineM(tv, sourceFrom = sourceFrom, targetFrom = targetFrom, max = max)
	
	# Extracts indices and values for distances grater or equal than threshold
	thrIdx <- which(tvcos >= threshold)
	re <- row(tvcos)[thrIdx] + sourceFrom - 1L
	ce <- col(tvcos)[thrIdx] + targetFrom - 1L
	we <- tvcos[thrIdx]
	
	# Because edges are undirected we want only edges corresponding
	# to the upper triangle of the distance matrix
	lowerIndx <- which(re > ce)
	return(list(source=re[lowerIndx], target=ce[lowerIndx], weights=we[lowerIndx]))
}

myCosineM <- function(tv, sourceFrom = 1L, targetFrom = 1L, max = 5000) {
	
	sourceTo <- min(sourceFrom + max - 1, nrow(tv))
	targetTo <- min(targetFrom + max - 1, nrow(tv))
	
	# Computes cosine distances between given tv rows
	tvxy <- tcrossprod(tv[sourceFrom:sourceTo,], tv[targetFrom:targetTo,])
	tvxx <- sqrProd(tv[sourceFrom:sourceTo,])
	tvyy <- sqrProd(tv[targetFrom:targetTo,])
	tvxxm <- matrix(rep(x=tvxx,times=ncol(tvxy)),ncol=ncol(tvxy))
	tvyym <- matrix(rep(x=tvyy,times=nrow(tvxy)),nrow=nrow(tvxy), byrow=TRUE)
		
	return(tvxy / sqrt(tvxxm * tvyym))
}

# Computes a symetric KL similarity from a KL Divergence
KLsim <- function (m, ...) {
  if (ncol(m) != 2) stop("m has to be a matrix with two columns, one per distribution")
  
  kds <- KLdiv(m,...)
  return(2/(kds[1,2] + kds[2,1]))
}

combineEdges <- function(x) {
	source <- unlist(lapply(x,FUN=function(e){e$source}))
	target <- unlist(lapply(x,FUN=function(e){e$target}))
	weight <- unlist(lapply(x,FUN=function(e){e$weight}))
	
	return(list(source = source, target = target,weight=weight))
}

myCosine <- function(x,y) {
	return(x %*% y / sqrt(x%*%x * y%*%y))
}


# Given a matrix X(n,m) with Xi i=1..n vectors as rows,
# it computes a vector of crossprod(Xi,Xi) i=1..n
sqrProd <- function(x) {
 	return (rowSums(x*x))
}

write.layout <- function(l,file) {
	write.table(l,file)
}

write.clusters <- function(c,clusterfile, centersfile) {
	write.table(c$cluster,clusterfile)
	write.table(c$centers,centersfile)
}

euclidDist <- function(x1,y1,x2,y2) {
	sqrt((x1-x2)^2 + (y1-y2)^2)
}

